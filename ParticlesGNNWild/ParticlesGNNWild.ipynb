{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb94292",
   "metadata": {},
   "source": [
    "# This notebook uses jet data to classify constituent particles (hadrons / not hadrons) using GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372a149",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41017d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.nn as tgnn\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torch_scatter import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff4e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import os\n",
    "import uproot\n",
    "import vector\n",
    "import awkward as ak\n",
    "vector.register_awkward()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb5103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up multithreading\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0159019",
   "metadata": {},
   "source": [
    "## Define dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af094d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetToPC(data.Dataset):\n",
    "    \"\"\"\n",
    "     This class returns jet data as set of point clouds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset_path:str,       # path to dataset\n",
    "        part_feats:list,        # list of particle features to use in training\n",
    "        jet_feats:list,         # list of global jet features to se in trainng\n",
    "        tree_name:str='tree',   # name of input data tree\n",
    "        k:int = 5,              # parameter for knn\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset = uproot.open(dataset_path)\n",
    "        self.tree = self.dataset[tree_name].arrays()\n",
    "        self.num_entries = self.dataset[tree_name].num_entries\n",
    "        \n",
    "        self.part_feats = part_feats\n",
    "        self.jet_feats = jet_feats\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        \n",
    "    def transform_jet_to_point_cloud(self, idx:int) -> dict :\n",
    "    \n",
    "        npart = self.tree['jet_nparticles'].to_numpy()[idx]  # get number of particles\n",
    "        \n",
    "        # setup feature lists\n",
    "        part_feat_list = np.stack([self.tree[part_feat][idx].to_numpy() for part_feat in self.part_feats]).T\n",
    "        jet_feat_list = np.stack([self.tree[jet_feat].to_numpy()[idx:idx+1] for jet_feat in self.jet_feats]).T\n",
    "        \n",
    "        # nan check\n",
    "        part_feat_list[np.isnan(part_feat_list)] = 0\n",
    "        jet_feat_list[np.isnan(jet_feat_list)] = 0\n",
    "        \n",
    "        # define target class\n",
    "        part_is_neutral_hadron = self.tree['part_isNeutralHadron'][idx].to_numpy()\n",
    "        part_is_charged_hadron = self.tree['part_isChargedHadron'][idx].to_numpy()\n",
    "        part_is_hadron = np.logical_or(part_is_neutral_hadron, part_is_charged_hadron)\n",
    "        \n",
    "        # set up knn graph\n",
    "        part_eta = torch.tensor(self.tree['part_deta'][idx].to_numpy())\n",
    "        part_phi = torch.tensor(self.tree['part_dphi'][idx].to_numpy())\n",
    "        eta_phi_pos = torch.stack([part_eta, part_phi], dim=-1)\n",
    "        \n",
    "        edge_index = tgnn.pool.knn_graph(x=eta_phi_pos, k=self.k) # this is saved as src, dst\n",
    "        \n",
    "        # set up edge distances (or edge weights)\n",
    "        src, dst = edge_index \n",
    "        part_del_eta = part_eta[dst] - part_eta[src]\n",
    "        part_del_phi = part_phi[dst] - part_phi[src]\n",
    "        \n",
    "        part_del_R = torch.hypot(part_del_eta, part_del_phi).view(-1, 1)\n",
    "        \n",
    "        # set up the data\n",
    "        data = Data (x=torch.tensor(part_feat_list), edge_index=edge_index, edge_deltaR=part_del_R)\n",
    "        data.label = torch.tensor(part_is_hadron)\n",
    "        data.global_data = torch.tensor(jet_feat_list)\n",
    "        data.seq_length = torch.tensor(int(npart))\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.num_entries\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> dict :\n",
    "        # Return the idx-th data point of the dataset\n",
    "        return self.transform_jet_to_point_cloud(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ed4da",
   "metadata": {},
   "source": [
    "## MLP builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d30c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildMLP(\n",
    "    in_features,\n",
    "    outputsize,\n",
    "    features,\n",
    "    add_batch_norm=False,\n",
    "    add_activation=None\n",
    "):\n",
    "    \"\"\"\n",
    "     This is a template function for a generalised MLP\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    \n",
    "    # input layer\n",
    "    layers.append(nn.Linear(in_features,features[0]))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    # hidden layers\n",
    "    for hidden_i in range(1,len(features)):\n",
    "        if add_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(features[hidden_i-1]))\n",
    "            \n",
    "        layers.append(nn.Linear(features[hidden_i-1],features[hidden_i]))\n",
    "        layers.append(nn.ReLU())\n",
    "    \n",
    "    # output layer\n",
    "    layers.append(nn.Linear(features[-1],outputsize))\n",
    "    \n",
    "    if add_activation!=None:\n",
    "        layers.append(add_activation)\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56926ef",
   "metadata": {},
   "source": [
    "## Set up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0816af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_edge_dim:int, \n",
    "        output_edge_dim:int, \n",
    "        node_dim:int, \n",
    "        global_dim:int, \n",
    "        features:list \n",
    "    ):\n",
    "        super(EdgeModel, self).__init__()\n",
    "        \n",
    "        self.edge_mlp = BuildMLP(\n",
    "            in_features=2*node_dim+global_dim+input_edge_dim, \n",
    "            outputsize=output_edge_dim, \n",
    "            features=features\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dst, edge_attr, u, edge_batch):\n",
    "        out = torch.cat([src, dst, edge_attr, u[edge_batch]], dim=1)\n",
    "        return self.edge_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0b9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_edge_dim:int, \n",
    "        input_node_dim:int, \n",
    "        output_node_dim:int, \n",
    "        input_global_dim:int, \n",
    "        features:list\n",
    "    ):\n",
    "        super(NodeModel, self).__init__()\n",
    "        \n",
    "        self.node_mlp = BuildMLP(\n",
    "            in_features=input_edge_dim+input_node_dim+input_global_dim, \n",
    "            outputsize=output_node_dim, \n",
    "            features=features\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "\n",
    "        row, col = edge_index\n",
    "        #out = torch.cat([x[row], edge_attr], dim=1)\n",
    "        #out = self.node_mlp_1(out)\n",
    "        out = scatter(edge_attr, col, dim=0, dim_size=x.size(0), reduce='mean')\n",
    "#         print('Agrregated out shape : ', out.shape)\n",
    "        out = torch.cat([x, out, u[batch]], dim=1)\n",
    "#         print('Stacked out shape : ', out.shape)\n",
    "        return self.node_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3264ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_edge_dim:int, \n",
    "        input_node_dim:int, \n",
    "        input_global_dim:int, \n",
    "        output_global_dim:int, \n",
    "        features:list\n",
    "    ):\n",
    "        super(GlobalModel, self).__init__()\n",
    "        \n",
    "        self.global_mlp = BuildMLP(\n",
    "            in_features=input_edge_dim+input_node_dim+input_global_dim, \n",
    "            outputsize=output_global_dim, \n",
    "            features=features\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        src_idx, dst_idx = edge_index\n",
    "        \n",
    "        out = torch.cat([\n",
    "            u,\n",
    "            scatter(x, batch, dim=0, reduce='mean'),\n",
    "            scatter(edge_attr, batch[src_idx], dim=0, reduce='mean')\n",
    "        ], dim=1)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1512c9",
   "metadata": {},
   "source": [
    "## Set up a single GNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acabfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildGNNLayer(\n",
    "    n_edge_input:int, \n",
    "    n_edge_hidden:list, \n",
    "    n_edge_output:int, \n",
    "    n_node_input:int, \n",
    "    n_node_hidden:list, \n",
    "    n_node_output:int,\n",
    "    n_global_input:int,\n",
    "    n_global_hidden:list,\n",
    "    n_global_output:int\n",
    "):\n",
    "    \n",
    "    edge_network = EdgeModel(n_edge_input, n_edge_output, n_node_input, n_global_input, n_edge_hidden)\n",
    "    node_network = NodeModel(n_edge_output, n_node_input, n_node_output, n_global_input, n_node_hidden)\n",
    "    global_network = GlobalModel(n_edge_output, n_node_output, n_global_input, n_global_output, n_global_hidden)\n",
    "    \n",
    "    gnn_layer = tgnn.MetaLayer(edge_model=edge_network, node_model=node_network, global_model=global_network)\n",
    "    \n",
    "    return gnn_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b99256",
   "metadata": {},
   "source": [
    "## Set up the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a927180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        nLayers:int,\n",
    "        edge_input_features:int,\n",
    "        node_input_features:int,\n",
    "        global_input_features:int,\n",
    "        edge_hidden_features:list,\n",
    "        node_hidden_features:list,\n",
    "        global_hidden_features:list,\n",
    "        edge_output_features:list,\n",
    "        node_output_features:list,\n",
    "        global_output_features:list,\n",
    "        normalization:str = '', \n",
    "        pool:str = 'mean'\n",
    "    ) -> None:\n",
    "        \n",
    "        \n",
    "        super(GNNModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        edge_output_features.insert(0, edge_input_features)\n",
    "        node_output_features.insert(0, node_input_features)\n",
    "        global_output_features.insert(0, global_input_features)\n",
    "        \n",
    "        # layer loop\n",
    "        for i in range(len(edge_hidden_features)):\n",
    "\n",
    "            layers.append(BuildGNNLayer(\n",
    "                edge_output_features[i], \n",
    "                edge_hidden_features[i],\n",
    "                edge_output_features[i+1], \n",
    "                node_output_features[i], \n",
    "                node_hidden_features[i], \n",
    "                node_output_features[i+1],\n",
    "                global_output_features[i],\n",
    "                global_hidden_features[i],\n",
    "                global_output_features[i+1]\n",
    "            ))\n",
    "        \n",
    "        \n",
    "        self.sequential = nn.ModuleList(layers)\n",
    "    \n",
    "       \n",
    "    def forward(\n",
    "        self, \n",
    "        x:torch.Tensor, \n",
    "        edge_attr:torch.Tensor, \n",
    "        u:torch.Tensor, \n",
    "        edge_index:torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        for i, layer in enumerate(self.sequential):\n",
    "            x, edge_attr, u = layer(x, edge_index, edge_attr, u, batch)\n",
    "            \n",
    "        out = F.log_softmax(x, dim=-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbc104",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f46df7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, labels, batch):\n",
    "        \n",
    "        # Get the unique batch indices\n",
    "        unique_batch_indices = torch.unique(batch, return_counts=True)\n",
    "\n",
    "        # Get the weighted mean of BCE for each jet\n",
    "        net_loss = 0\n",
    "        start_idx = 0\n",
    "        \n",
    "        for batch_idx, batch_size in zip(unique_batch_indices[0], unique_batch_indices[1]):\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            # Slice the output\n",
    "            separated_output = outputs[start_idx:end_idx]  \n",
    "            separated_label = labels[start_idx:end_idx]\n",
    "            \n",
    "            net_loss += F.nll_loss(separated_output, separated_label.type(torch.LongTensor))*batch_size\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        net_loss = net_loss/batch.shape[0]\n",
    "\n",
    "        return net_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b24b22",
   "metadata": {},
   "source": [
    "## Train and Test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a0afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training routine\n",
    "def Train(model, device, train_loader, optimizer, loss_fn):\n",
    "    \n",
    "    train_loss_ep = 0.\n",
    "    \n",
    "    # set the train mode for the model\n",
    "    model.train()\n",
    "    \n",
    "    # input batchwise data\n",
    "    with tqdm(train_loader, ascii=True) as tq:\n",
    "        for dl in tq:\n",
    "            x, edge_index, edge_attr, label, u, batch = (\n",
    "                dl.x.to(device), \n",
    "                dl.edge_index.to(device), \n",
    "                dl.edge_deltaR.to(device), \n",
    "                dl.label.to(device), \n",
    "                dl.global_data.to(device), \n",
    "                dl.batch.to(device)\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, edge_attr, u, edge_index, batch)\n",
    "            loss = loss_fn(output, label, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_ep += loss.item()*x.size(0)\n",
    "        \n",
    "    return train_loss_ep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641f5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing routine\n",
    "def Test(model, device, test_loader, loss_fn):\n",
    "    \n",
    "    test_loss_ep = 0.\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm(train_loader, ascii=True) as tq:\n",
    "        for dl in tq:\n",
    "            x, edge_index, edge_attr, label, u, batch = (\n",
    "                dl.x.to(device), \n",
    "                dl.edge_index.to(device), \n",
    "                dl.edge_deltaR.to(device), \n",
    "                dl.label.to(device), \n",
    "                dl.global_data.to(device), \n",
    "                dl.batch.to(device)\n",
    "            )\n",
    "            \n",
    "            output = model(x, edge_attr, u, edge_index, batch)\n",
    "            loss = loss_fn(output, label, batch)\n",
    "            \n",
    "            test_loss_ep += loss.item()*x.size(0)\n",
    "        \n",
    "    return test_loss_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba91608",
   "metadata": {},
   "source": [
    "## Define model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5048d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_features = [\n",
    "    'part_px',\n",
    "    'part_py',\n",
    "    'part_pz',\n",
    "    'part_energy',\n",
    "    'part_deta',\n",
    "    'part_dphi',\n",
    "    'part_d0val',\n",
    "    'part_d0err',\n",
    "    'part_dzval',\n",
    "    'part_dzerr',\n",
    "    'part_charge',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb7f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_features = [\n",
    "    'jet_pt',\n",
    "    'jet_eta',\n",
    "    'jet_phi',\n",
    "    'jet_energy',\n",
    "    'jet_tau1',\n",
    "    'jet_tau2',\n",
    "    'jet_tau3',\n",
    "    'jet_tau4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b2d2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_hidden_features = [[4, 10, 5],[5, 10, 5],[5, 10, 5]]\n",
    "node_hidden_features = [[20, 20, 20, 20],[20, 20, 20, 20],[20, 20, 20, 20]]\n",
    "global_hidden_features = [[8, 8, 8], [8, 8, 8], [8, 8, 8]]\n",
    "\n",
    "edge_output_features = [10, 10, 10]\n",
    "node_output_features = [10, 10, 2]\n",
    "global_output_features = [8, 8, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d6bed",
   "metadata": {},
   "source": [
    "## Runtime settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc818c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device to run\n",
    "device = 'cpu' # cuda, mps\n",
    "\n",
    "# dataset path\n",
    "dataset_path = '../data/JetClass_example_100k.root'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831f8cd",
   "metadata": {},
   "source": [
    "## Instantiate data loader, model, optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81266da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 5\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 5\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "train_size, valid_size = 0.6, 0.2\n",
    "\n",
    "# create train, test and valid split\n",
    "jet_dataset = JetToPC(dataset_path, particle_features, jet_features)\n",
    "num_train = len(jet_dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_split = int(np.floor(train_size * num_train))\n",
    "valid_split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index, test_index = indices[0:train_split], indices[train_split:train_split + valid_split], indices[train_split + valid_split:]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "test_sampler = SubsetRandomSampler(test_index)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=jet_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    sampler=train_sampler,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=jet_dataset, \n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    sampler=valid_sampler\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=jet_dataset, \n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,  \n",
    "    sampler=test_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05ce5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = GNNModel(\n",
    "            nLayers=len(edge_output_features), \n",
    "            edge_input_features=1,\n",
    "            node_input_features=len(particle_features),\n",
    "            global_input_features=len(jet_features),\n",
    "            edge_hidden_features=edge_hidden_features,\n",
    "            node_hidden_features=node_hidden_features,\n",
    "            global_hidden_features=global_hidden_features,\n",
    "            edge_output_features=edge_output_features,\n",
    "            node_output_features=node_output_features,\n",
    "            global_output_features=global_output_features\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e01f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate optimiser\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87a0163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate loss function\n",
    "loss_fn = CustomLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6bf05",
   "metadata": {},
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|#####################7                                                                        | 2780/12000 [01:43<06:13, 24.70it/s]"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    \n",
    "    train_loss = Train(model, device, train_loader, optimizer, loss_fn)\n",
    "    \n",
    "    valid_loss = Test(model, device, valid_loader, loss_fn)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), '../models/hadron_id.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602b747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
